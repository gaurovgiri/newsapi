name: Scrape News

permissions:
  contents: write

on:
  # Run every 4 hours
  schedule:
    - cron: '0 */4 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
  
  # Run on push to test
  push:
    branches: [ main, master ]
    paths:
      - 'scrape_news.py'
      - 'news_source.py'
      - 'sources/**'
      - '.github/workflows/scrape-news.yml'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create data directory
      run: mkdir -p data
    
    - name: Run news scraper
      run: python main.py
    
    - name: Check for changes
      id: check_changes
      run: |
        git diff --quiet data/ || echo "changes=true" >> $GITHUB_OUTPUT
    
    - name: Commit and push changes
      if: steps.check_changes.outputs.changes == 'true'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add data/
        git commit -m "üóûÔ∏è Update news data - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: news-data-${{ github.run_number }}
        path: data/
        retention-days: 7
